 #COMPANY: CODTECH IT SOLUTIONS

*NAME*:Kumkum C. Bhanarkar

*INTERN ID*: CT08DN279

*DOMAIN*: BIG DATA

*DURATION*: 8 WEEKS

*MENTOR*: NEELA SANTOSH

In this task, I used Apache Spark with Python (PySpark) to clean a large dataset. The goal was to handle common issues like missing values and duplicate records which often occur in real-world data. I created a PySpark script in Google Colab that read the dataset, checked each column for null or empty values, dropped rows with missing entries, and removed duplicate records. I also explored how to fill missing values using default or statistical methods. This exercise taught me how Spark handles distributed data cleaning at scale, which is much more efficient than traditional Python (Pandas) for large datasets.

#Output
<img width="1920" height="647" alt="Image" src="https://github.com/user-attachments/assets/9d0bab5b-83c1-4002-9e24-8318a02ce729" />
<img width="1920" height="154" alt="Image" src="https://github.com/user-attachments/assets/a15a39ca-5f3c-45fe-8556-78fb6ef88df1" />
<img width="1920" height="312" alt="Image" src="https://github.com/user-attachments/assets/16c44767-19dd-4b94-8b32-fc902ada6fa3" />
